{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Hybrid Recommender System\n",
                "\n",
                "This notebook combines collaborative filtering and content-based approaches for hybrid recommendations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
                "\n",
                "# Set display options\n",
                "pd.set_option('display.max_columns', None)\n",
                "sns.set_style('whitegrid')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Required Data and Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load user-item matrix\n",
                "# user_item_matrix = pd.read_csv('../results/user_item_matrix.csv', index_col=0)\n",
                "\n",
                "# Load similarity matrices from previous notebooks\n",
                "# user_similarity = pd.read_csv('../results/user_similarity.csv', index_col=0)\n",
                "# item_similarity_cf = pd.read_csv('../results/item_similarity.csv', index_col=0)\n",
                "# item_similarity_content = pd.read_csv('../results/content_item_similarity.csv', index_col=0)\n",
                "\n",
                "# print(\"Data loaded successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Weighted Hybrid Method"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def weighted_hybrid_recommendations(user_id, \n",
                "                                   collaborative_recs,\n",
                "                                   content_recs,\n",
                "                                   alpha=0.5,\n",
                "                                   top_n=10):\n",
                "    \"\"\"\n",
                "    Combine collaborative and content-based recommendations using weighted approach.\n",
                "    \n",
                "    Args:\n",
                "        user_id: Target user ID\n",
                "        collaborative_recs: DataFrame with collaborative filtering recommendations\n",
                "        content_recs: DataFrame with content-based recommendations\n",
                "        alpha: Weight for collaborative filtering (1-alpha for content-based)\n",
                "        top_n: Number of recommendations\n",
                "        \n",
                "    Returns:\n",
                "        DataFrame with hybrid recommendations\n",
                "    \"\"\"\n",
                "    # Normalize scores to [0, 1]\n",
                "    collab_scores = collaborative_recs.set_index('item_id')['score']\n",
                "    content_scores = content_recs.set_index('item_id')['score']\n",
                "    \n",
                "    # Normalize\n",
                "    collab_scores = (collab_scores - collab_scores.min()) / (collab_scores.max() - collab_scores.min())\n",
                "    content_scores = (content_scores - content_scores.min()) / (content_scores.max() - content_scores.min())\n",
                "    \n",
                "    # Combine scores\n",
                "    all_items = set(collab_scores.index) | set(content_scores.index)\n",
                "    hybrid_scores = {}\n",
                "    \n",
                "    for item in all_items:\n",
                "        collab_score = collab_scores.get(item, 0)\n",
                "        content_score = content_scores.get(item, 0)\n",
                "        hybrid_scores[item] = alpha * collab_score + (1 - alpha) * content_score\n",
                "    \n",
                "    # Sort and get top-N\n",
                "    sorted_items = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
                "    \n",
                "    recommendations = pd.DataFrame(sorted_items, columns=['item_id', 'hybrid_score'])\n",
                "    return recommendations\n",
                "\n",
                "# Example usage\n",
                "# Assuming you have collaborative_recs and content_recs from previous notebooks\n",
                "# hybrid_recs = weighted_hybrid_recommendations(user_id, collaborative_recs, content_recs, alpha=0.6)\n",
                "# print(\"Hybrid Recommendations:\")\n",
                "# print(hybrid_recs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Switching Hybrid Method"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def switching_hybrid_recommendations(user_id,\n",
                "                                    user_item_matrix,\n",
                "                                    collaborative_recs,\n",
                "                                    content_recs,\n",
                "                                    min_ratings=10,\n",
                "                                    top_n=10):\n",
                "    \"\"\"\n",
                "    Switch between collaborative and content-based based on user profile.\n",
                "    \n",
                "    Args:\n",
                "        user_id: Target user ID\n",
                "        user_item_matrix: User-item rating matrix\n",
                "        collaborative_recs: Collaborative filtering recommendations\n",
                "        content_recs: Content-based recommendations\n",
                "        min_ratings: Minimum ratings to use collaborative filtering\n",
                "        top_n: Number of recommendations\n",
                "        \n",
                "    Returns:\n",
                "        DataFrame with recommendations and method used\n",
                "    \"\"\"\n",
                "    # Check user's rating count\n",
                "    user_ratings = user_item_matrix.loc[user_id]\n",
                "    rating_count = (user_ratings > 0).sum()\n",
                "    \n",
                "    # Switch based on rating count\n",
                "    if rating_count >= min_ratings:\n",
                "        # Use collaborative filtering for active users\n",
                "        recommendations = collaborative_recs.head(top_n).copy()\n",
                "        recommendations['method'] = 'collaborative'\n",
                "    else:\n",
                "        # Use content-based for new/inactive users\n",
                "        recommendations = content_recs.head(top_n).copy()\n",
                "        recommendations['method'] = 'content-based'\n",
                "    \n",
                "    return recommendations\n",
                "\n",
                "# Example usage\n",
                "# switching_recs = switching_hybrid_recommendations(user_id, user_item_matrix, collaborative_recs, content_recs)\n",
                "# print(f\"\\nSwitching Hybrid Recommendations (Method: {switching_recs['method'].iloc[0]}):\")\n",
                "# print(switching_recs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Feature Augmentation Hybrid"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def feature_augmentation_hybrid(user_id,\n",
                "                               user_item_matrix,\n",
                "                               item_similarity_cf,\n",
                "                               item_similarity_content,\n",
                "                               top_n=10):\n",
                "    \"\"\"\n",
                "    Augment collaborative features with content features.\n",
                "    \n",
                "    Args:\n",
                "        user_id: Target user ID\n",
                "        user_item_matrix: User-item rating matrix\n",
                "        item_similarity_cf: Item similarity from collaborative filtering\n",
                "        item_similarity_content: Item similarity from content-based\n",
                "        top_n: Number of recommendations\n",
                "        \n",
                "    Returns:\n",
                "        DataFrame with recommendations\n",
                "    \"\"\"\n",
                "    user_ratings = user_item_matrix.loc[user_id]\n",
                "    rated_items = user_ratings[user_ratings > 0]\n",
                "    unrated_items = user_ratings[user_ratings == 0].index\n",
                "    \n",
                "    predictions = {}\n",
                "    \n",
                "    for item in unrated_items:\n",
                "        cf_score = 0\n",
                "        content_score = 0\n",
                "        \n",
                "        # Calculate collaborative score\n",
                "        for rated_item, rating in rated_items.items():\n",
                "            cf_score += item_similarity_cf.loc[item, rated_item] * rating\n",
                "            content_score += item_similarity_content.loc[item, rated_item] * rating\n",
                "        \n",
                "        # Combine scores (average)\n",
                "        if len(rated_items) > 0:\n",
                "            predictions[item] = (cf_score + content_score) / (2 * len(rated_items))\n",
                "    \n",
                "    # Get top-N recommendations\n",
                "    sorted_items = sorted(predictions.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
                "    recommendations = pd.DataFrame(sorted_items, columns=['item_id', 'augmented_score'])\n",
                "    \n",
                "    return recommendations\n",
                "\n",
                "# Example usage\n",
                "# augmented_recs = feature_augmentation_hybrid(user_id, user_item_matrix, item_similarity_cf, item_similarity_content)\n",
                "# print(\"\\nFeature Augmentation Hybrid Recommendations:\")\n",
                "# print(augmented_recs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluate Hybrid Approaches"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_hybrid_methods(test_users, methods_dict, actual_ratings):\n",
                "    \"\"\"\n",
                "    Evaluate different hybrid methods.\n",
                "    \n",
                "    Args:\n",
                "        test_users: List of user IDs to test\n",
                "        methods_dict: Dictionary of method names and their recommendation functions\n",
                "        actual_ratings: Actual ratings for evaluation\n",
                "        \n",
                "    Returns:\n",
                "        DataFrame with evaluation metrics\n",
                "    \"\"\"\n",
                "    results = {}\n",
                "    \n",
                "    for method_name, method_func in methods_dict.items():\n",
                "        predictions = []\n",
                "        actuals = []\n",
                "        \n",
                "        for user_id in test_users:\n",
                "            recs = method_func(user_id)\n",
                "            # Compare with actual ratings\n",
                "            # This is a simplified version\n",
                "            predictions.extend(recs['predicted_rating'].values if 'predicted_rating' in recs.columns else [])\n",
                "            actuals.extend(actual_ratings.loc[user_id, recs['item_id']].values)\n",
                "        \n",
                "        # Calculate metrics\n",
                "        if len(predictions) > 0:\n",
                "            rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
                "            mae = mean_absolute_error(actuals, predictions)\n",
                "            results[method_name] = {'RMSE': rmse, 'MAE': mae}\n",
                "    \n",
                "    return pd.DataFrame(results).T"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualize Hybrid Performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare different alpha values for weighted hybrid\n",
                "# alphas = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]\n",
                "# performance = []\n",
                "\n",
                "# for alpha in alphas:\n",
                "#     # Calculate performance for each alpha\n",
                "#     # This is a placeholder - implement actual evaluation\n",
                "#     pass\n",
                "\n",
                "# plt.figure(figsize=(10, 6))\n",
                "# plt.plot(alphas, performance, marker='o')\n",
                "# plt.xlabel('Alpha (Weight for Collaborative)')\n",
                "# plt.ylabel('Performance Metric')\n",
                "# plt.title('Hybrid Performance vs Alpha')\n",
                "# plt.grid(True, alpha=0.3)\n",
                "# plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save hybrid recommendations\n",
                "# hybrid_recs.to_csv('../results/hybrid_recommendations.csv', index=False)\n",
                "# print(\"Saved hybrid recommendations\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}